{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Sentiment Analysis - News Article Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time, os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chromedriver = \"/Applications/chromedriver\"\n",
    "os.environ['webdriver.chrome.driver'] = chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitcoin Magazine Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium driver\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get('https://bitcoinmagazine.com/articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_to_bottom(driver):\n",
    "    \"\"\"\n",
    "    Scroll to the bottom of a webpage.\n",
    "    Thank you to user53558 on github.\n",
    "    \"\"\"\n",
    "\n",
    "    old_position = 0\n",
    "    new_position = None\n",
    "\n",
    "    while new_position != old_position:\n",
    "        # Get old scroll position\n",
    "        old_position = driver.execute_script(\n",
    "                (\"return (window.pageYOffset !== undefined) ?\"\n",
    "                 \" window.pageYOffset : (document.documentElement ||\"\n",
    "                 \" document.body.parentNode || document.body);\"))\n",
    "        # Sleep and Scroll\n",
    "        time.sleep(1)\n",
    "        driver.execute_script((\n",
    "                \"var scrollingElement = (document.scrollingElement ||\"\n",
    "                \" document.body);scrollingElement.scrollTop =\"\n",
    "                \" scrollingElement.scrollHeight;\"))\n",
    "        # Get new position\n",
    "        new_position = driver.execute_script(\n",
    "                (\"return (window.pageYOffset !== undefined) ?\"\n",
    "                 \" window.pageYOffset : (document.documentElement ||\"\n",
    "                 \" document.body.parentNode || document.body);\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click \"See More\" button 100 times. Repeat this cell as much as necessary.\n",
    "for i in range(0, 100):\n",
    "    try:\n",
    "        scroll_to_bottom(driver)\n",
    "        time.sleep(1)\n",
    "        driver.find_element_by_xpath('/html/body/phoenix-page/div/div/div[2]/section[2]/section/div/button').click()\n",
    "    except:\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HTML after displaying 100 iterations of more articles\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all individual article links\n",
    "bitcoin_magazine_articles = soup.find_all('phoenix-super-link', 'href')\n",
    "titles = soup.find_all('phoenix-ellipsis', {'class' : 'm-ellipsis m-card--header'})\n",
    "bitcoin_magazine_articles = [h.find('a')['href'] for h in titles]\n",
    "bitcoin_magazine_articles = ['https://bitcoinmagazine.com/' + link for link in bitcoin_magazine_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the article list\n",
    "with open('sentiment_pickles/pickle_bitcoin_magazine.pickle', 'wb') as to_write:\n",
    "    pickle.dump(bitcoin_magazine_articles, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pickled links (if re-starting here)\n",
    "with open('sentiment_pickles/pickle_bitcoin_magazine.pickle', 'rb') as read_file:\n",
    "    bitcoin_magazine_links = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary for creating the dataframe\n",
    "bitcoin_magazine_dict = {'date' : [],\n",
    "                         'website' : [],\n",
    "                         'title' : [],\n",
    "                         'body' : []\n",
    "                        }\n",
    "\n",
    "# Loop through all collected links and save data to the dictionary\n",
    "for link in bitcoin_magazine_links:\n",
    "    try:\n",
    "        page = requests.get(link).text\n",
    "        soup = BeautifulSoup(page, 'html5lib')\n",
    "\n",
    "        title = soup.find('h1', {'class' : 'm-detail-header--title'}).getText()\n",
    "        \n",
    "        article_date = soup.find('phoenix-timeago').getText()\n",
    "        article_date = datetime.strptime(article_date, '%b %d, %Y').date()\n",
    "        \n",
    "        body = soup.find('div', {'class' : 'm-detail--body'}).find_all(['p', 'h2'])\n",
    "        body = '\\n'.join([b.getText() for b in body])\n",
    "        \n",
    "        bitcoin_magazine_dict['date'].append(article_date)\n",
    "        bitcoin_magazine_dict['website'].append('Bitcoin Magazine')\n",
    "        bitcoin_magazine_dict['title'].append(title)\n",
    "        bitcoin_magazine_dict['body'].append(body)\n",
    "    except:\n",
    "        # If there is a failure, skip and move to the next article.\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe\n",
    "bitcoin_magazine_df = pd.DataFrame(bitcoin_magazine_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the articles dataframe\n",
    "with open('sentiment_pickles/pickle_bitcoin_magazine_df.pickle', 'wb') as to_write:\n",
    "    pickle.dump(bitcoin_magazine_df, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CryptoSlate Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up list of links for CrypoSlate articles\n",
    "root = 'https://cryptoslate.com/news/bitcoin/page/'\n",
    "links = [root + str(x) +'/' for x in range(1, 174)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through links to get article links\n",
    "cryptoslate_links = []\n",
    "\n",
    "for link in links:\n",
    "    page = requests.get(link).text\n",
    "    soup = BeautifulSoup(page, 'html5lib')\n",
    "    \n",
    "    articles = soup.find('section', {'class': 'list-feed'}).find_all('a', href=True)\n",
    "    article_links = [a['href'] for a in articles][0:-1] # drop navigation link (next page)\n",
    "    \n",
    "    cryptoslate_links.extend(article_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the article links\n",
    "with open('sentiment_pickles/pickle_crypto_slate.pickle', 'wb') as to_write:\n",
    "    pickle.dump(cryptoslate_links, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pickled links\n",
    "with open('sentiment_pickles/pickle_crypto_slate.pickle', 'rb') as read_file:\n",
    "    cryptoslate_links = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary for creating the dataframe\n",
    "cryptoslate_dict = {'date' : [],\n",
    "                    'website' : [],\n",
    "                    'title' : [],\n",
    "                    'body' : []\n",
    "                   }\n",
    "\n",
    "# Loop through all collected links and save data to the dictionary\n",
    "for link in cryptoslate_links:\n",
    "    try:\n",
    "        page = requests.get(link).text\n",
    "        soup = BeautifulSoup(page, 'html5lib')\n",
    "\n",
    "        title = soup.find('h1', {'class' : 'post-title'}).getText()\n",
    "\n",
    "        article_date = soup.find('span', {'class' : 'post-date'}).getText()\n",
    "        article_date = ' '.join(article_date.split()[0:3])\n",
    "        article_date = datetime.strptime(article_date, '%B %d, %Y').date()\n",
    "\n",
    "        body = soup.find('div', {'class' : 'post-box clearfix'}).find('article').find_all(['p', 'h2'])\n",
    "        body = '\\n'.join([b.getText() for b in body])\n",
    "        \n",
    "        cryptoslate_dict['date'].append(article_date)\n",
    "        cryptoslate_dict['website'].append('CryptoSlate')\n",
    "        cryptoslate_dict['title'].append(title)\n",
    "        cryptoslate_dict['body'].append(body)\n",
    "    \n",
    "    except AttributeError:\n",
    "        # Body of the article is not found for premium articles\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe\n",
    "cryptoslate_df = pd.DataFrame(cryptoslate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the articles dataframe\n",
    "with open('sentiment_pickles/pickle_crypto_slate_df.pickle', 'wb') as to_write:\n",
    "    pickle.dump(cryptoslate_df, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitcoin News Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up list of links for Bitcoin News articles\n",
    "root = 'https://news.bitcoin.com/page/'\n",
    "links = [root + str(x) +'/' for x in range(2, 1630)] # Skip first page, different format and very recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through links to get article links\n",
    "bitcoin_news_links = []\n",
    "\n",
    "for link in links:\n",
    "    page = requests.get(link).texts\n",
    "    soup = BeautifulSoup(page, 'html5lib')\n",
    "    \n",
    "    articles = soup.find('div', {'class': 'td-container td-pb-article-list'}).find_all('div', {'class': 'story story--medium'})\n",
    "    article_links = [a.find('a')['href'] for a in articles]\n",
    "    \n",
    "    bitcoin_news_links.extend(article_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary for creating the dataframe\n",
    "bitcoin_news_dict = {'date' : [],\n",
    "                     'website' : [],\n",
    "                     'title' : [],\n",
    "                     'body' : []\n",
    "                    }\n",
    "\n",
    "# Loop through all collected links and save data to the dictionary\n",
    "for link in bitcoin_news_links:\n",
    "    try:\n",
    "        page = requests.get(link).text\n",
    "        soup = BeautifulSoup(page, 'html5lib')\n",
    "\n",
    "        article_space = soup.find('article', {'class' : 'article__body'})\n",
    "\n",
    "        title = article_space.find('h1', {'class' : 'article__header__heading'}).getText().strip()\n",
    "\n",
    "        article_date = soup.find('time', {'class' : 'article__info__date'}).getText().strip()\n",
    "        article_date = datetime.strptime(article_date, '%b %d, %Y').date()\n",
    "\n",
    "        body = article_space.find_all(['p', 'h2'])\n",
    "        body = '\\n'.join([b.getText() for b in body])\n",
    "        \n",
    "        bitcoin_news_dict['date'].append(article_date)\n",
    "        bitcoin_news_dict['website'].append('Bitcoin.com')\n",
    "        bitcoin_news_dict['title'].append(title)\n",
    "        bitcoin_news_dict['body'].append(body)\n",
    "    \n",
    "    except:\n",
    "        # If there is a failure, move to the next article.\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe\n",
    "bitcoin_news_df = pd.DataFrame(bitcoin_news_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the articles dataframe\n",
    "with open('sentiment_pickles/pickle_bitcoin_news_df.pickle', 'wb') as to_write:\n",
    "    pickle.dump(bitcoin_news_df, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coin Telegraph Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium driver\n",
    "opts = Options()\n",
    "opts.add_argument('user-agent=Mozilla/5.0')\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver, options=opts)\n",
    "driver.get('https://cointelegraph.com/tags/bitcoin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click \"See More\" button 1000 times. Repeat this cell as much as necessary.\n",
    "for i in range(0, 1000):\n",
    "    try:\n",
    "        scroll_to_bottom(driver)\n",
    "        time.sleep(1)\n",
    "        driver.find_element_by_xpath('/html/body/div/div/div/div[1]/main/div/div/div[2]/div/div[2]/div/div/button').click()\n",
    "    except:\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_telegraph_links = soup.find_all('a', {'class' : 'post-card-inline__title-link'})\n",
    "coin_telegraph_links = [a['href'] for a in coin_telegraph_links]\n",
    "coin_telegraph_links = ['https://cointelegraph.com' + a for a in coin_telegraph_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the article list\n",
    "with open('sentiment_pickles/pickle_coin_telegraph.pickle', 'wb') as to_write:\n",
    "    pickle.dump(coin_telegraph_links, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pickled links (if re-starting here)\n",
    "with open('sentiment_pickles/pickle_coin_telegraph.pickle', 'rb') as read_file:\n",
    "    coin_telegraph_links = pickle.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary for creating the dataframe\n",
    "coin_telegraph_dict = {'date' : [],\n",
    "                       'website' : [],\n",
    "                       'title' : [],\n",
    "                       'body' : []\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "# Loop through all collected links and save data to the dictionary\n",
    "for link in coin_telegraph_links:\n",
    "    try:\n",
    "        page = requests.get(link, headers=headers).text\n",
    "        soup = BeautifulSoup(page, 'html5lib')\n",
    "        \n",
    "        article_space = soup.find('article', {'class' : 'post__article'})\n",
    "        \n",
    "        title = article_space.find('h1', {'class' : 'post__title'}).getText().strip()\n",
    "        article_date = article_space.find('time')['datetime']\n",
    "        article_date = datetime.strptime(article_date, '%Y-%m-%d').date()\n",
    "        \n",
    "        body = article_space.find('div', {'class' : 'post-content'}).find_all(['p', 'h2'])\n",
    "        body = '\\n'.join([b.getText() for b in body])\n",
    "        \n",
    "        coin_telegraph_dict['date'].append(article_date)\n",
    "        coin_telegraph_dict['website'].append('Coin Telegraph')\n",
    "        coin_telegraph_dict['title'].append(title)\n",
    "        coin_telegraph_dict['body'].append(body)\n",
    "    except:\n",
    "        # If there is a failure, move to the next article.\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe\n",
    "coin_telegraph_df = pd.DataFrame(coin_telegraph_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the articles dataframe\n",
    "with open('sentiment_pickles/pickle_coin_telegraph_df.pickle', 'wb') as to_write:\n",
    "    pickle.dump(coin_telegraph_df, to_write)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
